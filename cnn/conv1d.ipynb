{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones_1d = np.ones(5)\n",
    "ones_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_1d = np.ones(3)\n",
    "weight_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "strides_1d = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(5)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_1d = tf.constant(ones_1d, dtype=tf.float32)\n",
    "input_1d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(3)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_1d = tf.constant(weight_1d, dtype=tf.float32)\n",
    "filter_1d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "input_width = int(input_1d.shape[0])\n",
    "filter_width = int(filter_1d.shape[0])\n",
    "print(input_width)\n",
    "print(filter_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function reshape in module tensorflow.python.ops.gen_array_ops:\n",
      "\n",
      "reshape(tensor, shape, name=None)\n",
      "    Reshapes a tensor.\n",
      "    \n",
      "    Given `tensor`, this operation returns a tensor that has the same values\n",
      "    as `tensor` with shape `shape`.\n",
      "    \n",
      "    If one component of `shape` is the special value -1, the size of that dimension\n",
      "    is computed so that the total size remains constant.  In particular, a `shape`\n",
      "    of `[-1]` flattens into 1-D.  At most one component of `shape` can be -1.\n",
      "    \n",
      "    If `shape` is 1-D or higher, then the operation returns a tensor with shape\n",
      "    `shape` filled with the values of `tensor`. In this case, the number of elements\n",
      "    implied by `shape` must be the same as the number of elements in `tensor`.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    ```\n",
      "    # tensor 't' is [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "    # tensor 't' has shape [9]\n",
      "    reshape(t, [3, 3]) ==> [[1, 2, 3],\n",
      "                            [4, 5, 6],\n",
      "                            [7, 8, 9]]\n",
      "    \n",
      "    # tensor 't' is [[[1, 1], [2, 2]],\n",
      "    #                [[3, 3], [4, 4]]]\n",
      "    # tensor 't' has shape [2, 2, 2]\n",
      "    reshape(t, [2, 4]) ==> [[1, 1, 2, 2],\n",
      "                            [3, 3, 4, 4]]\n",
      "    \n",
      "    # tensor 't' is [[[1, 1, 1],\n",
      "    #                 [2, 2, 2]],\n",
      "    #                [[3, 3, 3],\n",
      "    #                 [4, 4, 4]],\n",
      "    #                [[5, 5, 5],\n",
      "    #                 [6, 6, 6]]]\n",
      "    # tensor 't' has shape [3, 2, 3]\n",
      "    # pass '[-1]' to flatten 't'\n",
      "    reshape(t, [-1]) ==> [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6]\n",
      "    \n",
      "    # -1 can also be used to infer the shape\n",
      "    \n",
      "    # -1 is inferred to be 9:\n",
      "    reshape(t, [2, -1]) ==> [[1, 1, 1, 2, 2, 2, 3, 3, 3],\n",
      "                             [4, 4, 4, 5, 5, 5, 6, 6, 6]]\n",
      "    # -1 is inferred to be 2:\n",
      "    reshape(t, [-1, 9]) ==> [[1, 1, 1, 2, 2, 2, 3, 3, 3],\n",
      "                             [4, 4, 4, 5, 5, 5, 6, 6, 6]]\n",
      "    # -1 is inferred to be 3:\n",
      "    reshape(t, [ 2, -1, 3]) ==> [[[1, 1, 1],\n",
      "                                  [2, 2, 2],\n",
      "                                  [3, 3, 3]],\n",
      "                                 [[4, 4, 4],\n",
      "                                  [5, 5, 5],\n",
      "                                  [6, 6, 6]]]\n",
      "    \n",
      "    # tensor 't' is [7]\n",
      "    # shape `[]` reshapes to a scalar\n",
      "    reshape(t, []) ==> 7\n",
      "    ```\n",
      "    \n",
      "    Args:\n",
      "      tensor: A `Tensor`.\n",
      "      shape: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
      "        Defines the shape of the output tensor.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor`. Has the same type as `tensor`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5, 1)\n",
      "(3, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "input_rsh_1d = tf.reshape(input_1d, [1, input_width, 1])\n",
    "kernel_rsh_1d = tf.reshape(filter_1d, [filter_width, 1, 1])\n",
    "print(input_rsh_1d.shape)\n",
    "print(kernel_rsh_1d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function conv1d in module tensorflow.python.ops.nn_ops:\n",
      "\n",
      "conv1d(value, filters, stride, padding, use_cudnn_on_gpu=None, data_format=None, name=None)\n",
      "    Computes a 1-D convolution given 3-D input and filter tensors. (deprecated argument values) (deprecated argument values)\n",
      "    \n",
      "    Warning: SOME ARGUMENT VALUES ARE DEPRECATED: `(data_format='NCHW')`. They will be removed in a future version.\n",
      "    Instructions for updating:\n",
      "    `NCHW` for data_format is deprecated, use `NCW` instead\n",
      "    \n",
      "    Warning: SOME ARGUMENT VALUES ARE DEPRECATED: `(data_format='NHWC')`. They will be removed in a future version.\n",
      "    Instructions for updating:\n",
      "    `NHWC` for data_format is deprecated, use `NWC` instead\n",
      "    \n",
      "    Given an input tensor of shape\n",
      "      [batch, in_width, in_channels]\n",
      "    if data_format is \"NWC\", or\n",
      "      [batch, in_channels, in_width]\n",
      "    if data_format is \"NCW\",\n",
      "    and a filter / kernel tensor of shape\n",
      "    [filter_width, in_channels, out_channels], this op reshapes\n",
      "    the arguments to pass them to conv2d to perform the equivalent\n",
      "    convolution operation.\n",
      "    \n",
      "    Internally, this op reshapes the input tensors and invokes `tf.nn.conv2d`.\n",
      "    For example, if `data_format` does not start with \"NC\", a tensor of shape\n",
      "      [batch, in_width, in_channels]\n",
      "    is reshaped to\n",
      "      [batch, 1, in_width, in_channels],\n",
      "    and the filter is reshaped to\n",
      "      [1, filter_width, in_channels, out_channels].\n",
      "    The result is then reshaped back to\n",
      "      [batch, out_width, out_channels]\n",
      "    \\(where out_width is a function of the stride and padding as in conv2d\\) and\n",
      "    returned to the caller.\n",
      "    \n",
      "    Args:\n",
      "      value: A 3D `Tensor`.  Must be of type `float16`, `float32`, or `float64`.\n",
      "      filters: A 3D `Tensor`.  Must have the same type as `value`.\n",
      "      stride: An `integer`.  The number of entries by which\n",
      "        the filter is moved right at each step.\n",
      "      padding: 'SAME' or 'VALID'\n",
      "      use_cudnn_on_gpu: An optional `bool`.  Defaults to `True`.\n",
      "      data_format: An optional `string` from `\"NWC\", \"NCW\"`.  Defaults\n",
      "        to `\"NWC\"`, the data is stored in the order of\n",
      "        [batch, in_width, in_channels].  The `\"NCW\"` format stores\n",
      "        data as [batch, in_channels, in_width].\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor`.  Has the same type as input.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: if `data_format` is invalid.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.nn.conv1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function squeeze in module tensorflow.python.ops.array_ops:\n",
      "\n",
      "squeeze(input, axis=None, name=None, squeeze_dims=None)\n",
      "    Removes dimensions of size 1 from the shape of a tensor. (deprecated arguments)\n",
      "    \n",
      "    Warning: SOME ARGUMENTS ARE DEPRECATED: `(squeeze_dims)`. They will be removed in a future version.\n",
      "    Instructions for updating:\n",
      "    Use the `axis` argument instead\n",
      "    \n",
      "    Given a tensor `input`, this operation returns a tensor of the same type with\n",
      "    all dimensions of size 1 removed. If you don't want to remove all size 1\n",
      "    dimensions, you can remove specific size 1 dimensions by specifying\n",
      "    `axis`.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    ```python\n",
      "    # 't' is a tensor of shape [1, 2, 1, 3, 1, 1]\n",
      "    tf.shape(tf.squeeze(t))  # [2, 3]\n",
      "    ```\n",
      "    \n",
      "    Or, to remove specific size 1 dimensions:\n",
      "    \n",
      "    ```python\n",
      "    # 't' is a tensor of shape [1, 2, 1, 3, 1, 1]\n",
      "    tf.shape(tf.squeeze(t, [2, 4]))  # [1, 2, 3, 1]\n",
      "    ```\n",
      "    \n",
      "    Args:\n",
      "      input: A `Tensor`. The `input` to squeeze.\n",
      "      axis: An optional list of `ints`. Defaults to `[]`.\n",
      "        If specified, only squeezes the dimensions listed. The dimension\n",
      "        index starts at 0. It is an error to squeeze a dimension that is not 1.\n",
      "        Must be in the range `[-rank(input), rank(input))`.\n",
      "      name: A name for the operation (optional).\n",
      "      squeeze_dims: Deprecated keyword argument that is now axis.\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor`. Has the same type as `input`.\n",
      "      Contains the same data as `input`, but has one or more dimensions of\n",
      "      size 1 removed.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: When both `squeeze_dims` and `axis` are specified.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.squeeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Tensor in module tensorflow.python.framework.ops:\n",
      "\n",
      "class Tensor(_TensorLike)\n",
      " |  Tensor(op, value_index, dtype)\n",
      " |  \n",
      " |  Represents one of the outputs of an `Operation`.\n",
      " |  \n",
      " |  A `Tensor` is a symbolic handle to one of the outputs of an\n",
      " |  `Operation`. It does not hold the values of that operation's output,\n",
      " |  but instead provides a means of computing those values in a\n",
      " |  TensorFlow `tf.Session`.\n",
      " |  \n",
      " |  This class has two primary purposes:\n",
      " |  \n",
      " |  1. A `Tensor` can be passed as an input to another `Operation`.\n",
      " |     This builds a dataflow connection between operations, which\n",
      " |     enables TensorFlow to execute an entire `Graph` that represents a\n",
      " |     large, multi-step computation.\n",
      " |  \n",
      " |  2. After the graph has been launched in a session, the value of the\n",
      " |     `Tensor` can be computed by passing it to\n",
      " |     `tf.Session.run`.\n",
      " |     `t.eval()` is a shortcut for calling\n",
      " |     `tf.get_default_session().run(t)`.\n",
      " |  \n",
      " |  In the following example, `c`, `d`, and `e` are symbolic `Tensor`\n",
      " |  objects, whereas `result` is a numpy array that stores a concrete\n",
      " |  value:\n",
      " |  \n",
      " |  ```python\n",
      " |  # Build a dataflow graph.\n",
      " |  c = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
      " |  d = tf.constant([[1.0, 1.0], [0.0, 1.0]])\n",
      " |  e = tf.matmul(c, d)\n",
      " |  \n",
      " |  # Construct a `Session` to execute the graph.\n",
      " |  sess = tf.Session()\n",
      " |  \n",
      " |  # Execute the graph and store the value that `e` represents in `result`.\n",
      " |  result = sess.run(e)\n",
      " |  ```\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Tensor\n",
      " |      _TensorLike\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __abs__ = abs(x, name=None)\n",
      " |      Computes the absolute value of a tensor.\n",
      " |      \n",
      " |      Given a tensor `x` of complex numbers, this operation returns a tensor of type\n",
      " |      `float32` or `float64` that is the absolute value of each element in `x`. All\n",
      " |      elements in `x` must be complex numbers of the form \\\\(a + bj\\\\). The\n",
      " |      absolute value is computed as \\\\( \\sqrt{a^2 + b^2}\\\\).  For example:\n",
      " |      ```python\n",
      " |      x = tf.constant([[-2.25 + 4.75j], [-3.25 + 5.75j]])\n",
      " |      tf.abs(x)  # [5.25594902, 6.60492229]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` or `SparseTensor` of type `float16`, `float32`, `float64`,\n",
      " |          `int32`, `int64`, `complex64` or `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` or `SparseTensor` the same size and type as `x` with absolute\n",
      " |          values.\n",
      " |        Note, for `complex64` or `complex128` input, the returned `Tensor` will be\n",
      " |          of type `float32` or `float64`, respectively.\n",
      " |      \n",
      " |        If `x` is a `SparseTensor`, returns\n",
      " |        `SparseTensor(x.indices, tf.math.abs(x.values, ...), x.dense_shape)`\n",
      " |  \n",
      " |  __add__ = binary_op_wrapper(x, y)\n",
      " |      Returns x + y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.add` supports broadcasting. `AddN` does not. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`, `string`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __and__ = binary_op_wrapper(x, y)\n",
      " |      Returns the truth value of x AND y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.logical_and` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __bool__(self)\n",
      " |      Dummy method to prevent a tensor from being used as a Python `bool`.\n",
      " |      \n",
      " |      This overload raises a `TypeError` when the user inadvertently\n",
      " |      treats a `Tensor` as a boolean (e.g. in an `if` statement). For\n",
      " |      example:\n",
      " |      \n",
      " |      ```python\n",
      " |      if tf.constant(True):  # Will raise.\n",
      " |        # ...\n",
      " |      \n",
      " |      if tf.constant(5) < tf.constant(7):  # Will raise.\n",
      " |        # ...\n",
      " |      ```\n",
      " |      \n",
      " |      This disallows ambiguities between testing the Python value vs testing the\n",
      " |      dynamic condition of the `Tensor`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        `TypeError`.\n",
      " |  \n",
      " |  __copy__(self)\n",
      " |  \n",
      " |  __div__ = binary_op_wrapper(x, y)\n",
      " |      Divide two values using Python 2 semantics. Used for Tensor.__div__.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      Returns:\n",
      " |        `x / y` returns the quotient of x and y.\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __floordiv__ = binary_op_wrapper(x, y)\n",
      " |      Divides `x / y` elementwise, rounding toward the most negative integer.\n",
      " |      \n",
      " |      The same as `tf.div(x,y)` for integers, but uses `tf.floor(tf.div(x,y))` for\n",
      " |      floating point arguments so that the result is always an integer (though\n",
      " |      possibly an integer represented as floating point).  This op is generated by\n",
      " |      `x // y` floor division in Python 3 and in Python 2.7 with\n",
      " |      `from __future__ import division`.\n",
      " |      \n",
      " |      `x` and `y` must have the same type, and the result will have the same type\n",
      " |      as well.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        `x / y` rounded down.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If the inputs are complex.\n",
      " |  \n",
      " |  __ge__ = greater_equal(x, y, name=None)\n",
      " |      Returns the truth value of (x >= y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.greater_equal` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __getitem__ = _slice_helper(tensor, slice_spec, var=None)\n",
      " |      Overload for Tensor.__getitem__.\n",
      " |      \n",
      " |      This operation extracts the specified region from the tensor.\n",
      " |      The notation is similar to NumPy with the restriction that\n",
      " |      currently only support basic indexing. That means that\n",
      " |      using a non-scalar tensor as input is not currently allowed.\n",
      " |      \n",
      " |      Some useful examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      # strip leading and trailing 2 elements\n",
      " |      foo = tf.constant([1,2,3,4,5,6])\n",
      " |      print(foo[2:-2].eval())  # => [3,4]\n",
      " |      \n",
      " |      # skip every row and reverse every column\n",
      " |      foo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])\n",
      " |      print(foo[::2,::-1].eval())  # => [[3,2,1], [9,8,7]]\n",
      " |      \n",
      " |      # Use scalar tensors as indices on both dimensions\n",
      " |      print(foo[tf.constant(0), tf.constant(2)].eval())  # => 3\n",
      " |      \n",
      " |      # Insert another dimension\n",
      " |      foo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])\n",
      " |      print(foo[tf.newaxis, :, :].eval()) # => [[[1,2,3], [4,5,6], [7,8,9]]]\n",
      " |      print(foo[:, tf.newaxis, :].eval()) # => [[[1,2,3]], [[4,5,6]], [[7,8,9]]]\n",
      " |      print(foo[:, :, tf.newaxis].eval()) # => [[[1],[2],[3]], [[4],[5],[6]],\n",
      " |      [[7],[8],[9]]]\n",
      " |      \n",
      " |      # Ellipses (3 equivalent operations)\n",
      " |      foo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])\n",
      " |      print(foo[tf.newaxis, :, :].eval())  # => [[[1,2,3], [4,5,6], [7,8,9]]]\n",
      " |      print(foo[tf.newaxis, ...].eval())  # => [[[1,2,3], [4,5,6], [7,8,9]]]\n",
      " |      print(foo[tf.newaxis].eval())  # => [[[1,2,3], [4,5,6], [7,8,9]]]\n",
      " |      ```\n",
      " |      \n",
      " |      Notes:\n",
      " |        - `tf.newaxis` is `None` as in NumPy.\n",
      " |        - An implicit ellipsis is placed at the end of the `slice_spec`\n",
      " |        - NumPy advanced indexing is currently not supported.\n",
      " |      \n",
      " |      Args:\n",
      " |        tensor: An ops.Tensor object.\n",
      " |        slice_spec: The arguments to Tensor.__getitem__.\n",
      " |        var: In the case of variable slice assignment, the Variable\n",
      " |          object to slice (i.e. tensor is the read-only view of this\n",
      " |          variable).\n",
      " |      \n",
      " |      Returns:\n",
      " |        The appropriate slice of \"tensor\", based on \"slice_spec\".\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If a slice range is negative size.\n",
      " |        TypeError: If the slice indices aren't int, slice, ellipsis,\n",
      " |          tf.newaxis or scalar int32/int64 tensors.\n",
      " |  \n",
      " |  __gt__ = greater(x, y, name=None)\n",
      " |      Returns the truth value of (x > y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.greater` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __hash__(self)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __init__(self, op, value_index, dtype)\n",
      " |      Creates a new `Tensor`.\n",
      " |      \n",
      " |      Args:\n",
      " |        op: An `Operation`. `Operation` that computes this tensor.\n",
      " |        value_index: An `int`. Index of the operation's endpoint that produces\n",
      " |          this tensor.\n",
      " |        dtype: A `DType`. Type of elements stored in this tensor.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If the op is not an `Operation`.\n",
      " |  \n",
      " |  __invert__ = logical_not(x, name=None)\n",
      " |      Returns the truth value of NOT x element-wise.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |  \n",
      " |  __le__ = less_equal(x, y, name=None)\n",
      " |      Returns the truth value of (x <= y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.less_equal` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __lt__ = less(x, y, name=None)\n",
      " |      Returns the truth value of (x < y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.less` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __matmul__ = binary_op_wrapper(x, y)\n",
      " |      Multiplies matrix `a` by matrix `b`, producing `a` * `b`.\n",
      " |      \n",
      " |      The inputs must, following any transpositions, be tensors of rank >= 2\n",
      " |      where the inner 2 dimensions specify valid matrix multiplication arguments,\n",
      " |      and any further outer dimensions match.\n",
      " |      \n",
      " |      Both matrices must be of the same type. The supported types are:\n",
      " |      `float16`, `float32`, `float64`, `int32`, `complex64`, `complex128`.\n",
      " |      \n",
      " |      Either matrix can be transposed or adjointed (conjugated and transposed) on\n",
      " |      the fly by setting one of the corresponding flag to `True`. These are `False`\n",
      " |      by default.\n",
      " |      \n",
      " |      If one or both of the matrices contain a lot of zeros, a more efficient\n",
      " |      multiplication algorithm can be used by setting the corresponding\n",
      " |      `a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.\n",
      " |      This optimization is only available for plain matrices (rank-2 tensors) with\n",
      " |      datatypes `bfloat16` or `float32`.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      # 2-D tensor `a`\n",
      " |      # [[1, 2, 3],\n",
      " |      #  [4, 5, 6]]\n",
      " |      a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n",
      " |      \n",
      " |      # 2-D tensor `b`\n",
      " |      # [[ 7,  8],\n",
      " |      #  [ 9, 10],\n",
      " |      #  [11, 12]]\n",
      " |      b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])\n",
      " |      \n",
      " |      # `a` * `b`\n",
      " |      # [[ 58,  64],\n",
      " |      #  [139, 154]]\n",
      " |      c = tf.matmul(a, b)\n",
      " |      \n",
      " |      \n",
      " |      # 3-D tensor `a`\n",
      " |      # [[[ 1,  2,  3],\n",
      " |      #   [ 4,  5,  6]],\n",
      " |      #  [[ 7,  8,  9],\n",
      " |      #   [10, 11, 12]]]\n",
      " |      a = tf.constant(np.arange(1, 13, dtype=np.int32),\n",
      " |                      shape=[2, 2, 3])\n",
      " |      \n",
      " |      # 3-D tensor `b`\n",
      " |      # [[[13, 14],\n",
      " |      #   [15, 16],\n",
      " |      #   [17, 18]],\n",
      " |      #  [[19, 20],\n",
      " |      #   [21, 22],\n",
      " |      #   [23, 24]]]\n",
      " |      b = tf.constant(np.arange(13, 25, dtype=np.int32),\n",
      " |                      shape=[2, 3, 2])\n",
      " |      \n",
      " |      # `a` * `b`\n",
      " |      # [[[ 94, 100],\n",
      " |      #   [229, 244]],\n",
      " |      #  [[508, 532],\n",
      " |      #   [697, 730]]]\n",
      " |      c = tf.matmul(a, b)\n",
      " |      \n",
      " |      # Since python >= 3.5 the @ operator is supported (see PEP 465).\n",
      " |      # In TensorFlow, it simply calls the `tf.matmul()` function, so the\n",
      " |      # following lines are equivalent:\n",
      " |      d = a @ b @ [[10.], [11.]]\n",
      " |      d = tf.matmul(tf.matmul(a, b), [[10.], [11.]])\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        a: `Tensor` of type `float16`, `float32`, `float64`, `int32`, `complex64`,\n",
      " |          `complex128` and rank > 1.\n",
      " |        b: `Tensor` with same type and rank as `a`.\n",
      " |        transpose_a: If `True`, `a` is transposed before multiplication.\n",
      " |        transpose_b: If `True`, `b` is transposed before multiplication.\n",
      " |        adjoint_a: If `True`, `a` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        adjoint_b: If `True`, `b` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        a_is_sparse: If `True`, `a` is treated as a sparse matrix.\n",
      " |        b_is_sparse: If `True`, `b` is treated as a sparse matrix.\n",
      " |        name: Name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of the same type as `a` and `b` where each inner-most matrix is\n",
      " |        the product of the corresponding matrices in `a` and `b`, e.g. if all\n",
      " |        transpose or adjoint attributes are `False`:\n",
      " |      \n",
      " |        `output`[..., i, j] = sum_k (`a`[..., i, k] * `b`[..., k, j]),\n",
      " |        for all indices i, j.\n",
      " |      \n",
      " |        Note: This is matrix product, not element-wise product.\n",
      " |      \n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If transpose_a and adjoint_a, or transpose_b and adjoint_b\n",
      " |          are both set to True.\n",
      " |  \n",
      " |  __mod__ = binary_op_wrapper(x, y)\n",
      " |      Returns element-wise remainder of division. When `x < 0` xor `y < 0` is\n",
      " |      \n",
      " |      true, this follows Python semantics in that the result here is consistent\n",
      " |      with a flooring divide. E.g. `floor(x / y) * y + mod(x, y) = x`.\n",
      " |      \n",
      " |      *NOTE*: `floormod` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `int32`, `int64`, `bfloat16`, `half`, `float32`, `float64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __mul__ = binary_op_wrapper(x, y)\n",
      " |      Dispatches cwise mul for \"Dense*Dense\" and \"Dense*Sparse\".\n",
      " |  \n",
      " |  __neg__ = neg(x, name=None)\n",
      " |      Computes numerical negative value element-wise.\n",
      " |      \n",
      " |      I.e., \\\\(y = -x\\\\).\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int32`, `int64`, `complex64`, `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |      \n",
      " |        If `x` is a `SparseTensor`, returns\n",
      " |        `SparseTensor(x.indices, tf.math.negative(x.values, ...), x.dense_shape)`\n",
      " |  \n",
      " |  __nonzero__(self)\n",
      " |      Dummy method to prevent a tensor from being used as a Python `bool`.\n",
      " |      \n",
      " |      This is the Python 2.x counterpart to `__bool__()` above.\n",
      " |      \n",
      " |      Raises:\n",
      " |        `TypeError`.\n",
      " |  \n",
      " |  __or__ = binary_op_wrapper(x, y)\n",
      " |      Returns the truth value of x OR y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.logical_or` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __pow__ = binary_op_wrapper(x, y)\n",
      " |      Computes the power of one value to another.\n",
      " |      \n",
      " |      Given a tensor `x` and a tensor `y`, this operation computes \\\\(x^y\\\\) for\n",
      " |      corresponding elements in `x` and `y`. For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([[2, 2], [3, 3]])\n",
      " |      y = tf.constant([[8, 16], [2, 3]])\n",
      " |      tf.pow(x, y)  # [[256, 65536], [9, 27]]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
      " |         `complex64`, or `complex128`.\n",
      " |        y: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
      " |         `complex64`, or `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`.\n",
      " |  \n",
      " |  __radd__ = r_binary_op_wrapper(y, x)\n",
      " |      Returns x + y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.add` supports broadcasting. `AddN` does not. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`, `string`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __rand__ = r_binary_op_wrapper(y, x)\n",
      " |      Returns the truth value of x AND y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.logical_and` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __rdiv__ = r_binary_op_wrapper(y, x)\n",
      " |      Divide two values using Python 2 semantics. Used for Tensor.__div__.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      Returns:\n",
      " |        `x / y` returns the quotient of x and y.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __rfloordiv__ = r_binary_op_wrapper(y, x)\n",
      " |      Divides `x / y` elementwise, rounding toward the most negative integer.\n",
      " |      \n",
      " |      The same as `tf.div(x,y)` for integers, but uses `tf.floor(tf.div(x,y))` for\n",
      " |      floating point arguments so that the result is always an integer (though\n",
      " |      possibly an integer represented as floating point).  This op is generated by\n",
      " |      `x // y` floor division in Python 3 and in Python 2.7 with\n",
      " |      `from __future__ import division`.\n",
      " |      \n",
      " |      `x` and `y` must have the same type, and the result will have the same type\n",
      " |      as well.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        `x / y` rounded down.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If the inputs are complex.\n",
      " |  \n",
      " |  __rmatmul__ = r_binary_op_wrapper(y, x)\n",
      " |      Multiplies matrix `a` by matrix `b`, producing `a` * `b`.\n",
      " |      \n",
      " |      The inputs must, following any transpositions, be tensors of rank >= 2\n",
      " |      where the inner 2 dimensions specify valid matrix multiplication arguments,\n",
      " |      and any further outer dimensions match.\n",
      " |      \n",
      " |      Both matrices must be of the same type. The supported types are:\n",
      " |      `float16`, `float32`, `float64`, `int32`, `complex64`, `complex128`.\n",
      " |      \n",
      " |      Either matrix can be transposed or adjointed (conjugated and transposed) on\n",
      " |      the fly by setting one of the corresponding flag to `True`. These are `False`\n",
      " |      by default.\n",
      " |      \n",
      " |      If one or both of the matrices contain a lot of zeros, a more efficient\n",
      " |      multiplication algorithm can be used by setting the corresponding\n",
      " |      `a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.\n",
      " |      This optimization is only available for plain matrices (rank-2 tensors) with\n",
      " |      datatypes `bfloat16` or `float32`.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      # 2-D tensor `a`\n",
      " |      # [[1, 2, 3],\n",
      " |      #  [4, 5, 6]]\n",
      " |      a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n",
      " |      \n",
      " |      # 2-D tensor `b`\n",
      " |      # [[ 7,  8],\n",
      " |      #  [ 9, 10],\n",
      " |      #  [11, 12]]\n",
      " |      b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])\n",
      " |      \n",
      " |      # `a` * `b`\n",
      " |      # [[ 58,  64],\n",
      " |      #  [139, 154]]\n",
      " |      c = tf.matmul(a, b)\n",
      " |      \n",
      " |      \n",
      " |      # 3-D tensor `a`\n",
      " |      # [[[ 1,  2,  3],\n",
      " |      #   [ 4,  5,  6]],\n",
      " |      #  [[ 7,  8,  9],\n",
      " |      #   [10, 11, 12]]]\n",
      " |      a = tf.constant(np.arange(1, 13, dtype=np.int32),\n",
      " |                      shape=[2, 2, 3])\n",
      " |      \n",
      " |      # 3-D tensor `b`\n",
      " |      # [[[13, 14],\n",
      " |      #   [15, 16],\n",
      " |      #   [17, 18]],\n",
      " |      #  [[19, 20],\n",
      " |      #   [21, 22],\n",
      " |      #   [23, 24]]]\n",
      " |      b = tf.constant(np.arange(13, 25, dtype=np.int32),\n",
      " |                      shape=[2, 3, 2])\n",
      " |      \n",
      " |      # `a` * `b`\n",
      " |      # [[[ 94, 100],\n",
      " |      #   [229, 244]],\n",
      " |      #  [[508, 532],\n",
      " |      #   [697, 730]]]\n",
      " |      c = tf.matmul(a, b)\n",
      " |      \n",
      " |      # Since python >= 3.5 the @ operator is supported (see PEP 465).\n",
      " |      # In TensorFlow, it simply calls the `tf.matmul()` function, so the\n",
      " |      # following lines are equivalent:\n",
      " |      d = a @ b @ [[10.], [11.]]\n",
      " |      d = tf.matmul(tf.matmul(a, b), [[10.], [11.]])\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        a: `Tensor` of type `float16`, `float32`, `float64`, `int32`, `complex64`,\n",
      " |          `complex128` and rank > 1.\n",
      " |        b: `Tensor` with same type and rank as `a`.\n",
      " |        transpose_a: If `True`, `a` is transposed before multiplication.\n",
      " |        transpose_b: If `True`, `b` is transposed before multiplication.\n",
      " |        adjoint_a: If `True`, `a` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        adjoint_b: If `True`, `b` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        a_is_sparse: If `True`, `a` is treated as a sparse matrix.\n",
      " |        b_is_sparse: If `True`, `b` is treated as a sparse matrix.\n",
      " |        name: Name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of the same type as `a` and `b` where each inner-most matrix is\n",
      " |        the product of the corresponding matrices in `a` and `b`, e.g. if all\n",
      " |        transpose or adjoint attributes are `False`:\n",
      " |      \n",
      " |        `output`[..., i, j] = sum_k (`a`[..., i, k] * `b`[..., k, j]),\n",
      " |        for all indices i, j.\n",
      " |      \n",
      " |        Note: This is matrix product, not element-wise product.\n",
      " |      \n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If transpose_a and adjoint_a, or transpose_b and adjoint_b\n",
      " |          are both set to True.\n",
      " |  \n",
      " |  __rmod__ = r_binary_op_wrapper(y, x)\n",
      " |      Returns element-wise remainder of division. When `x < 0` xor `y < 0` is\n",
      " |      \n",
      " |      true, this follows Python semantics in that the result here is consistent\n",
      " |      with a flooring divide. E.g. `floor(x / y) * y + mod(x, y) = x`.\n",
      " |      \n",
      " |      *NOTE*: `floormod` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `int32`, `int64`, `bfloat16`, `half`, `float32`, `float64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __rmul__ = r_binary_op_wrapper(y, x)\n",
      " |      Dispatches cwise mul for \"Dense*Dense\" and \"Dense*Sparse\".\n",
      " |  \n",
      " |  __ror__ = r_binary_op_wrapper(y, x)\n",
      " |      Returns the truth value of x OR y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.logical_or` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __rpow__ = r_binary_op_wrapper(y, x)\n",
      " |      Computes the power of one value to another.\n",
      " |      \n",
      " |      Given a tensor `x` and a tensor `y`, this operation computes \\\\(x^y\\\\) for\n",
      " |      corresponding elements in `x` and `y`. For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([[2, 2], [3, 3]])\n",
      " |      y = tf.constant([[8, 16], [2, 3]])\n",
      " |      tf.pow(x, y)  # [[256, 65536], [9, 27]]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
      " |         `complex64`, or `complex128`.\n",
      " |        y: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
      " |         `complex64`, or `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`.\n",
      " |  \n",
      " |  __rsub__ = r_binary_op_wrapper(y, x)\n",
      " |      Returns x - y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `Subtract` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __rtruediv__ = r_binary_op_wrapper(y, x)\n",
      " |  \n",
      " |  __rxor__ = r_binary_op_wrapper(y, x)\n",
      " |      x ^ y = (x | y) & ~(x & y).\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  __sub__ = binary_op_wrapper(x, y)\n",
      " |      Returns x - y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `Subtract` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __truediv__ = binary_op_wrapper(x, y)\n",
      " |  \n",
      " |  __xor__ = binary_op_wrapper(x, y)\n",
      " |      x ^ y = (x | y) & ~(x & y).\n",
      " |  \n",
      " |  consumers(self)\n",
      " |      Returns a list of `Operation`s that consume this tensor.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of `Operation`s.\n",
      " |  \n",
      " |  eval(self, feed_dict=None, session=None)\n",
      " |      Evaluates this tensor in a `Session`.\n",
      " |      \n",
      " |      Calling this method will execute all preceding operations that\n",
      " |      produce the inputs needed for the operation that produces this\n",
      " |      tensor.\n",
      " |      \n",
      " |      *N.B.* Before invoking `Tensor.eval()`, its graph must have been\n",
      " |      launched in a session, and either a default session must be\n",
      " |      available, or `session` must be specified explicitly.\n",
      " |      \n",
      " |      Args:\n",
      " |        feed_dict: A dictionary that maps `Tensor` objects to feed values.\n",
      " |          See `tf.Session.run` for a\n",
      " |          description of the valid feed values.\n",
      " |        session: (Optional.) The `Session` to be used to evaluate this tensor. If\n",
      " |          none, the default session will be used.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A numpy array corresponding to the value of this tensor.\n",
      " |  \n",
      " |  get_shape(self)\n",
      " |      Alias of Tensor.shape.\n",
      " |  \n",
      " |  set_shape(self, shape)\n",
      " |      Updates the shape of this tensor.\n",
      " |      \n",
      " |      This method can be called multiple times, and will merge the given\n",
      " |      `shape` with the current shape of this tensor. It can be used to\n",
      " |      provide additional information about the shape of this tensor that\n",
      " |      cannot be inferred from the graph alone. For example, this can be used\n",
      " |      to provide additional information about the shapes of images:\n",
      " |      \n",
      " |      ```python\n",
      " |      _, image_data = tf.TFRecordReader(...).read(...)\n",
      " |      image = tf.image.decode_png(image_data, channels=3)\n",
      " |      \n",
      " |      # The height and width dimensions of `image` are data dependent, and\n",
      " |      # cannot be computed without executing the op.\n",
      " |      print(image.shape)\n",
      " |      ==> TensorShape([Dimension(None), Dimension(None), Dimension(3)])\n",
      " |      \n",
      " |      # We know that each image in this dataset is 28 x 28 pixels.\n",
      " |      image.set_shape([28, 28, 3])\n",
      " |      print(image.shape)\n",
      " |      ==> TensorShape([Dimension(28), Dimension(28), Dimension(3)])\n",
      " |      ```\n",
      " |      \n",
      " |      NOTE: This shape is not enforced at runtime. Setting incorrect shapes can\n",
      " |      result in inconsistencies between the statically-known graph and the runtime\n",
      " |      value of tensors. For runtime validation of the shape, use `tf.ensure_shape`\n",
      " |      instead.\n",
      " |      \n",
      " |      Args:\n",
      " |        shape: A `TensorShape` representing the shape of this tensor, a\n",
      " |        `TensorShapeProto`, a list, a tuple, or None.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If `shape` is not compatible with the current shape of\n",
      " |          this tensor.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  device\n",
      " |      The name of the device on which this tensor will be produced, or None.\n",
      " |  \n",
      " |  dtype\n",
      " |      The `DType` of elements in this tensor.\n",
      " |  \n",
      " |  graph\n",
      " |      The `Graph` that contains this tensor.\n",
      " |  \n",
      " |  name\n",
      " |      The string name of this tensor.\n",
      " |  \n",
      " |  op\n",
      " |      The `Operation` that produces this tensor as an output.\n",
      " |  \n",
      " |  shape\n",
      " |      Returns the `TensorShape` that represents the shape of this tensor.\n",
      " |      \n",
      " |      The shape is computed using shape inference functions that are\n",
      " |      registered in the Op for each `Operation`.  See\n",
      " |      `tf.TensorShape`\n",
      " |      for more details of what a shape represents.\n",
      " |      \n",
      " |      The inferred shape of a tensor is used to provide shape\n",
      " |      information without having to launch the graph in a session. This\n",
      " |      can be used for debugging, and providing early error messages. For\n",
      " |      example:\n",
      " |      \n",
      " |      ```python\n",
      " |      c = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
      " |      \n",
      " |      print(c.shape)\n",
      " |      ==> TensorShape([Dimension(2), Dimension(3)])\n",
      " |      \n",
      " |      d = tf.constant([[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]])\n",
      " |      \n",
      " |      print(d.shape)\n",
      " |      ==> TensorShape([Dimension(4), Dimension(2)])\n",
      " |      \n",
      " |      # Raises a ValueError, because `c` and `d` do not have compatible\n",
      " |      # inner dimensions.\n",
      " |      e = tf.matmul(c, d)\n",
      " |      \n",
      " |      f = tf.matmul(c, d, transpose_a=True, transpose_b=True)\n",
      " |      \n",
      " |      print(f.shape)\n",
      " |      ==> TensorShape([Dimension(3), Dimension(4)])\n",
      " |      ```\n",
      " |      \n",
      " |      In some cases, the inferred shape may have unknown dimensions. If\n",
      " |      the caller has additional information about the values of these\n",
      " |      dimensions, `Tensor.set_shape()` can be used to augment the\n",
      " |      inferred shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `TensorShape` representing the shape of this tensor.\n",
      " |  \n",
      " |  value_index\n",
      " |      The index of this tensor in the outputs of its `Operation`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  OVERLOADABLE_OPERATORS = {'__abs__', '__add__', '__and__', '__div__', ...\n",
      " |  \n",
      " |  __array_priority__ = 100\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from _TensorLike:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "squeeze_dims[0] not in [-1,1). for 'Squeeze_2' (op: 'Squeeze') with input shapes: [6].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1658\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1659\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1660\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: squeeze_dims[0] not in [-1,1). for 'Squeeze_2' (op: 'Squeeze') with input shapes: [6].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-87279f91cdf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36msqueeze\u001b[0;34m(input, axis, name, squeeze_dims)\u001b[0m\n\u001b[1;32m   3144\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3145\u001b[0m     \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3146\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36msqueeze\u001b[0;34m(input, axis, name)\u001b[0m\n\u001b[1;32m   9041\u001b[0m   \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"axis\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9042\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 9043\u001b[0;31m         \"Squeeze\", input=input, squeeze_dims=axis, name=name)\n\u001b[0m\u001b[1;32m   9044\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9045\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    786\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    787\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    789\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3298\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3299\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3300\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3301\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3302\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1821\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1822\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1823\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m     \u001b[0;31m# Initialize self._outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1660\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1661\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1662\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1664\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: squeeze_dims[0] not in [-1,1). for 'Squeeze_2' (op: 'Squeeze') with input shapes: [6]."
     ]
    }
   ],
   "source": [
    "tf.squeeze(tf.constant([1,2,1,3,1,1]), [2,4]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(6)])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.squeeze(tf.constant([1,2,1,3,1,1])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape:0\", shape=(1, 5, 1), dtype=float32) Tensor(\"Reshape_1:0\", shape=(3, 1, 1), dtype=float32) 1\n"
     ]
    }
   ],
   "source": [
    "print(input_rsh_1d, kernel_rsh_1d, strides_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_1d = tf.nn.conv1d(input_rsh_1d, kernel_rsh_1d, strides_1d, padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_1d # (1,5,1)\n",
    "output_sqz_1d = tf.squeeze(output_1d) # (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5, 1)\n",
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "print(output_1d.shape)\n",
    "print(output_sqz_1d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]]]\n",
      "[2. 3. 3. 3. 2.]\n"
     ]
    }
   ],
   "source": [
    "print( sess.run(output_1d) )\n",
    "print( sess.run(output_sqz_1d) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sqz.1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Graph in module tensorflow.python.framework.ops object:\n",
      "\n",
      "class Graph(builtins.object)\n",
      " |  A TensorFlow computation, represented as a dataflow graph.\n",
      " |  \n",
      " |  A `Graph` contains a set of\n",
      " |  `tf.Operation` objects,\n",
      " |  which represent units of computation; and\n",
      " |  `tf.Tensor` objects, which represent\n",
      " |  the units of data that flow between operations.\n",
      " |  \n",
      " |  A default `Graph` is always registered, and accessible by calling\n",
      " |  `tf.get_default_graph`.\n",
      " |  To add an operation to the default graph, simply call one of the functions\n",
      " |  that defines a new `Operation`:\n",
      " |  \n",
      " |  ```python\n",
      " |  c = tf.constant(4.0)\n",
      " |  assert c.graph is tf.get_default_graph()\n",
      " |  ```\n",
      " |  \n",
      " |  Another typical usage involves the\n",
      " |  `tf.Graph.as_default`\n",
      " |  context manager, which overrides the current default graph for the\n",
      " |  lifetime of the context:\n",
      " |  \n",
      " |  ```python\n",
      " |  g = tf.Graph()\n",
      " |  with g.as_default():\n",
      " |    # Define operations and tensors in `g`.\n",
      " |    c = tf.constant(30.0)\n",
      " |    assert c.graph is g\n",
      " |  ```\n",
      " |  \n",
      " |  Important note: This class *is not* thread-safe for graph construction. All\n",
      " |  operations should be created from a single thread, or external\n",
      " |  synchronization must be provided. Unless otherwise specified, all methods\n",
      " |  are not thread-safe.\n",
      " |  \n",
      " |  A `Graph` instance supports an arbitrary number of \"collections\"\n",
      " |  that are identified by name. For convenience when building a large\n",
      " |  graph, collections can store groups of related objects: for\n",
      " |  example, the `tf.Variable` uses a collection (named\n",
      " |  `tf.GraphKeys.GLOBAL_VARIABLES`) for\n",
      " |  all variables that are created during the construction of a graph. The caller\n",
      " |  may define additional collections by specifying a new name.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self)\n",
      " |      Creates a new, empty Graph.\n",
      " |  \n",
      " |  add_to_collection(self, name, value)\n",
      " |      Stores `value` in the collection with the given `name`.\n",
      " |      \n",
      " |      Note that collections are not sets, so it is possible to add a value to\n",
      " |      a collection several times.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: The key for the collection. The `GraphKeys` class\n",
      " |          contains many standard names for collections.\n",
      " |        value: The value to add to the collection.\n",
      " |  \n",
      " |  add_to_collections(self, names, value)\n",
      " |      Stores `value` in the collections given by `names`.\n",
      " |      \n",
      " |      Note that collections are not sets, so it is possible to add a value to\n",
      " |      a collection several times. This function makes sure that duplicates in\n",
      " |      `names` are ignored, but it will not check for pre-existing membership of\n",
      " |      `value` in any of the collections in `names`.\n",
      " |      \n",
      " |      `names` can be any iterable, but if `names` is a string, it is treated as a\n",
      " |      single collection name.\n",
      " |      \n",
      " |      Args:\n",
      " |        names: The keys for the collections to add to. The `GraphKeys` class\n",
      " |          contains many standard names for collections.\n",
      " |        value: The value to add to the collections.\n",
      " |  \n",
      " |  as_default(self)\n",
      " |      Returns a context manager that makes this `Graph` the default graph.\n",
      " |      \n",
      " |      This method should be used if you want to create multiple graphs\n",
      " |      in the same process. For convenience, a global default graph is\n",
      " |      provided, and all ops will be added to this graph if you do not\n",
      " |      create a new graph explicitly.\n",
      " |      \n",
      " |      Use this method with the `with` keyword to specify that ops created within\n",
      " |      the scope of a block should be added to this graph. In this case, once\n",
      " |      the scope of the `with` is exited, the previous default graph is set again\n",
      " |      as default. There is a stack, so it's ok to have multiple nested levels\n",
      " |      of `as_default` calls.\n",
      " |      \n",
      " |      The default graph is a property of the current thread. If you\n",
      " |      create a new thread, and wish to use the default graph in that\n",
      " |      thread, you must explicitly add a `with g.as_default():` in that\n",
      " |      thread's function.\n",
      " |      \n",
      " |      The following code examples are equivalent:\n",
      " |      \n",
      " |      ```python\n",
      " |      # 1. Using Graph.as_default():\n",
      " |      g = tf.Graph()\n",
      " |      with g.as_default():\n",
      " |        c = tf.constant(5.0)\n",
      " |        assert c.graph is g\n",
      " |      \n",
      " |      # 2. Constructing and making default:\n",
      " |      with tf.Graph().as_default() as g:\n",
      " |        c = tf.constant(5.0)\n",
      " |        assert c.graph is g\n",
      " |      ```\n",
      " |      \n",
      " |      If eager execution is enabled ops created under this context manager will be\n",
      " |      added to the graph instead of executed eagerly.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A context manager for using this graph as the default graph.\n",
      " |  \n",
      " |  as_graph_def(self, from_version=None, add_shapes=False)\n",
      " |      Returns a serialized `GraphDef` representation of this graph.\n",
      " |      \n",
      " |      The serialized `GraphDef` can be imported into another `Graph`\n",
      " |      (using `tf.import_graph_def`) or used with the\n",
      " |      [C++ Session API](../../api_docs/cc/index.md).\n",
      " |      \n",
      " |      This method is thread-safe.\n",
      " |      \n",
      " |      Args:\n",
      " |        from_version: Optional.  If this is set, returns a `GraphDef`\n",
      " |          containing only the nodes that were added to this graph since\n",
      " |          its `version` property had the given value.\n",
      " |        add_shapes: If true, adds an \"_output_shapes\" list attr to each\n",
      " |          node with the inferred shapes of each of its outputs.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A\n",
      " |        [`GraphDef`](https://www.tensorflow.org/code/tensorflow/core/framework/graph.proto)\n",
      " |        protocol buffer.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If the `graph_def` would be too large.\n",
      " |  \n",
      " |  as_graph_element(self, obj, allow_tensor=True, allow_operation=True)\n",
      " |      Returns the object referred to by `obj`, as an `Operation` or `Tensor`.\n",
      " |      \n",
      " |      This function validates that `obj` represents an element of this\n",
      " |      graph, and gives an informative error message if it is not.\n",
      " |      \n",
      " |      This function is the canonical way to get/validate an object of\n",
      " |      one of the allowed types from an external argument reference in the\n",
      " |      Session API.\n",
      " |      \n",
      " |      This method may be called concurrently from multiple threads.\n",
      " |      \n",
      " |      Args:\n",
      " |        obj: A `Tensor`, an `Operation`, or the name of a tensor or operation.\n",
      " |          Can also be any object with an `_as_graph_element()` method that returns\n",
      " |          a value of one of these types.\n",
      " |        allow_tensor: If true, `obj` may refer to a `Tensor`.\n",
      " |        allow_operation: If true, `obj` may refer to an `Operation`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The `Tensor` or `Operation` in the Graph corresponding to `obj`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `obj` is not a type we support attempting to convert\n",
      " |          to types.\n",
      " |        ValueError: If `obj` is of an appropriate type but invalid. For\n",
      " |          example, an invalid string.\n",
      " |        KeyError: If `obj` is not an object in the graph.\n",
      " |  \n",
      " |  clear_collection(self, name)\n",
      " |      Clears all values in a collection.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: The key for the collection. The `GraphKeys` class contains many\n",
      " |          standard names for collections.\n",
      " |  \n",
      " |  colocate_with(self, op, ignore_existing=False)\n",
      " |      Returns a context manager that specifies an op to colocate with.\n",
      " |      \n",
      " |      Note: this function is not for public use, only for internal libraries.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      a = tf.Variable([1.0])\n",
      " |      with g.colocate_with(a):\n",
      " |        b = tf.constant(1.0)\n",
      " |        c = tf.add(a, b)\n",
      " |      ```\n",
      " |      \n",
      " |      `b` and `c` will always be colocated with `a`, no matter where `a`\n",
      " |      is eventually placed.\n",
      " |      \n",
      " |      **NOTE** Using a colocation scope resets any existing device constraints.\n",
      " |      \n",
      " |      If `op` is `None` then `ignore_existing` must be `True` and the new\n",
      " |      scope resets all colocation and device constraints.\n",
      " |      \n",
      " |      Args:\n",
      " |        op: The op to colocate all created ops with, or `None`.\n",
      " |        ignore_existing: If true, only applies colocation of this op within\n",
      " |          the context, rather than applying all colocation properties\n",
      " |          on the stack.  If `op` is `None`, this value must be `True`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if op is None but ignore_existing is False.\n",
      " |      \n",
      " |      Yields:\n",
      " |        A context manager that specifies the op with which to colocate\n",
      " |        newly created ops.\n",
      " |  \n",
      " |  container(self, container_name)\n",
      " |      Returns a context manager that specifies the resource container to use.\n",
      " |      \n",
      " |      Stateful operations, such as variables and queues, can maintain their\n",
      " |      states on devices so that they can be shared by multiple processes.\n",
      " |      A resource container is a string name under which these stateful\n",
      " |      operations are tracked. These resources can be released or cleared\n",
      " |      with `tf.Session.reset()`.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      with g.container('experiment0'):\n",
      " |        # All stateful Operations constructed in this context will be placed\n",
      " |        # in resource container \"experiment0\".\n",
      " |        v1 = tf.Variable([1.0])\n",
      " |        v2 = tf.Variable([2.0])\n",
      " |        with g.container(\"experiment1\"):\n",
      " |          # All stateful Operations constructed in this context will be\n",
      " |          # placed in resource container \"experiment1\".\n",
      " |          v3 = tf.Variable([3.0])\n",
      " |          q1 = tf.FIFOQueue(10, tf.float32)\n",
      " |        # All stateful Operations constructed in this context will be\n",
      " |        # be created in the \"experiment0\".\n",
      " |        v4 = tf.Variable([4.0])\n",
      " |        q1 = tf.FIFOQueue(20, tf.float32)\n",
      " |        with g.container(\"\"):\n",
      " |          # All stateful Operations constructed in this context will be\n",
      " |          # be placed in the default resource container.\n",
      " |          v5 = tf.Variable([5.0])\n",
      " |          q3 = tf.FIFOQueue(30, tf.float32)\n",
      " |      \n",
      " |      # Resets container \"experiment0\", after which the state of v1, v2, v4, q1\n",
      " |      # will become undefined (such as uninitialized).\n",
      " |      tf.Session.reset(target, [\"experiment0\"])\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        container_name: container name string.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A context manager for defining resource containers for stateful ops,\n",
      " |          yields the container name.\n",
      " |  \n",
      " |  control_dependencies(self, control_inputs)\n",
      " |      Returns a context manager that specifies control dependencies.\n",
      " |      \n",
      " |      Use with the `with` keyword to specify that all operations constructed\n",
      " |      within the context should have control dependencies on\n",
      " |      `control_inputs`. For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      with g.control_dependencies([a, b, c]):\n",
      " |        # `d` and `e` will only run after `a`, `b`, and `c` have executed.\n",
      " |        d = ...\n",
      " |        e = ...\n",
      " |      ```\n",
      " |      \n",
      " |      Multiple calls to `control_dependencies()` can be nested, and in\n",
      " |      that case a new `Operation` will have control dependencies on the union\n",
      " |      of `control_inputs` from all active contexts.\n",
      " |      \n",
      " |      ```python\n",
      " |      with g.control_dependencies([a, b]):\n",
      " |        # Ops constructed here run after `a` and `b`.\n",
      " |        with g.control_dependencies([c, d]):\n",
      " |          # Ops constructed here run after `a`, `b`, `c`, and `d`.\n",
      " |      ```\n",
      " |      \n",
      " |      You can pass None to clear the control dependencies:\n",
      " |      \n",
      " |      ```python\n",
      " |      with g.control_dependencies([a, b]):\n",
      " |        # Ops constructed here run after `a` and `b`.\n",
      " |        with g.control_dependencies(None):\n",
      " |          # Ops constructed here run normally, not waiting for either `a` or `b`.\n",
      " |          with g.control_dependencies([c, d]):\n",
      " |            # Ops constructed here run after `c` and `d`, also not waiting\n",
      " |            # for either `a` or `b`.\n",
      " |      ```\n",
      " |      \n",
      " |      *N.B.* The control dependencies context applies *only* to ops that\n",
      " |      are constructed within the context. Merely using an op or tensor\n",
      " |      in the context does not add a control dependency. The following\n",
      " |      example illustrates this point:\n",
      " |      \n",
      " |      ```python\n",
      " |      # WRONG\n",
      " |      def my_func(pred, tensor):\n",
      " |        t = tf.matmul(tensor, tensor)\n",
      " |        with tf.control_dependencies([pred]):\n",
      " |          # The matmul op is created outside the context, so no control\n",
      " |          # dependency will be added.\n",
      " |          return t\n",
      " |      \n",
      " |      # RIGHT\n",
      " |      def my_func(pred, tensor):\n",
      " |        with tf.control_dependencies([pred]):\n",
      " |          # The matmul op is created in the context, so a control dependency\n",
      " |          # will be added.\n",
      " |          return tf.matmul(tensor, tensor)\n",
      " |      ```\n",
      " |      \n",
      " |      Also note that though execution of ops created under this scope will trigger\n",
      " |      execution of the dependencies, the ops created under this scope might still\n",
      " |      be pruned from a normal tensorflow graph. For example, in the following\n",
      " |      snippet of code the dependencies are never executed:\n",
      " |      \n",
      " |      ```python\n",
      " |        loss = model.loss()\n",
      " |        with tf.control_dependencies(dependencies):\n",
      " |          loss = loss + tf.constant(1)  # note: dependencies ignored in the\n",
      " |                                        # backward pass\n",
      " |        return tf.gradients(loss, model.variables)\n",
      " |      ```\n",
      " |      \n",
      " |      This is because evaluating the gradient graph does not require evaluating\n",
      " |      the constant(1) op created in the forward pass.\n",
      " |      \n",
      " |      Args:\n",
      " |        control_inputs: A list of `Operation` or `Tensor` objects which\n",
      " |          must be executed or computed before running the operations\n",
      " |          defined in the context.  Can also be `None` to clear the control\n",
      " |          dependencies.\n",
      " |      \n",
      " |      Returns:\n",
      " |       A context manager that specifies control dependencies for all\n",
      " |       operations constructed within the context.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `control_inputs` is not a list of `Operation` or\n",
      " |          `Tensor` objects.\n",
      " |  \n",
      " |  create_op(self, op_type, inputs, dtypes, input_types=None, name=None, attrs=None, op_def=None, compute_shapes=True, compute_device=True)\n",
      " |      Creates an `Operation` in this graph. (deprecated arguments)\n",
      " |      \n",
      " |      Warning: SOME ARGUMENTS ARE DEPRECATED: `(compute_shapes)`. They will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Shapes are always computed; don't use the compute_shapes as it has no effect.\n",
      " |      \n",
      " |      This is a low-level interface for creating an `Operation`. Most\n",
      " |      programs will not call this method directly, and instead use the\n",
      " |      Python op constructors, such as `tf.constant()`, which add ops to\n",
      " |      the default graph.\n",
      " |      \n",
      " |      Args:\n",
      " |        op_type: The `Operation` type to create. This corresponds to the\n",
      " |          `OpDef.name` field for the proto that defines the operation.\n",
      " |        inputs: A list of `Tensor` objects that will be inputs to the `Operation`.\n",
      " |        dtypes: A list of `DType` objects that will be the types of the tensors\n",
      " |          that the operation produces.\n",
      " |        input_types: (Optional.) A list of `DType`s that will be the types of\n",
      " |          the tensors that the operation consumes. By default, uses the base\n",
      " |          `DType` of each input in `inputs`. Operations that expect\n",
      " |          reference-typed inputs must specify `input_types` explicitly.\n",
      " |        name: (Optional.) A string name for the operation. If not specified, a\n",
      " |          name is generated based on `op_type`.\n",
      " |        attrs: (Optional.) A dictionary where the key is the attribute name (a\n",
      " |          string) and the value is the respective `attr` attribute of the\n",
      " |          `NodeDef` proto that will represent the operation (an `AttrValue`\n",
      " |          proto).\n",
      " |        op_def: (Optional.) The `OpDef` proto that describes the `op_type` that\n",
      " |          the operation will have.\n",
      " |        compute_shapes: (Optional.) Deprecated. Has no effect (shapes are always\n",
      " |          computed).\n",
      " |        compute_device: (Optional.) If True, device functions will be executed\n",
      " |          to compute the device property of the Operation.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: if any of the inputs is not a `Tensor`.\n",
      " |        ValueError: if colocation conflicts with existing device assignment.\n",
      " |      \n",
      " |      Returns:\n",
      " |        An `Operation` object.\n",
      " |  \n",
      " |  device(self, device_name_or_function)\n",
      " |      Returns a context manager that specifies the default device to use.\n",
      " |      \n",
      " |      The `device_name_or_function` argument may either be a device name\n",
      " |      string, a device function, or None:\n",
      " |      \n",
      " |      * If it is a device name string, all operations constructed in\n",
      " |        this context will be assigned to the device with that name, unless\n",
      " |        overridden by a nested `device()` context.\n",
      " |      * If it is a function, it will be treated as a function from\n",
      " |        Operation objects to device name strings, and invoked each time\n",
      " |        a new Operation is created. The Operation will be assigned to\n",
      " |        the device with the returned name.\n",
      " |      * If it is None, all `device()` invocations from the enclosing context\n",
      " |        will be ignored.\n",
      " |      \n",
      " |      For information about the valid syntax of device name strings, see\n",
      " |      the documentation in\n",
      " |      [`DeviceNameUtils`](https://www.tensorflow.org/code/tensorflow/core/util/device_name_utils.h).\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      with g.device('/device:GPU:0'):\n",
      " |        # All operations constructed in this context will be placed\n",
      " |        # on GPU 0.\n",
      " |        with g.device(None):\n",
      " |          # All operations constructed in this context will have no\n",
      " |          # assigned device.\n",
      " |      \n",
      " |      # Defines a function from `Operation` to device string.\n",
      " |      def matmul_on_gpu(n):\n",
      " |        if n.type == \"MatMul\":\n",
      " |          return \"/device:GPU:0\"\n",
      " |        else:\n",
      " |          return \"/cpu:0\"\n",
      " |      \n",
      " |      with g.device(matmul_on_gpu):\n",
      " |        # All operations of type \"MatMul\" constructed in this context\n",
      " |        # will be placed on GPU 0; all other operations will be placed\n",
      " |        # on CPU 0.\n",
      " |      ```\n",
      " |      \n",
      " |      **N.B.** The device scope may be overridden by op wrappers or\n",
      " |      other library code. For example, a variable assignment op\n",
      " |      `v.assign()` must be colocated with the `tf.Variable` `v`, and\n",
      " |      incompatible device scopes will be ignored.\n",
      " |      \n",
      " |      Args:\n",
      " |        device_name_or_function: The device name or function to use in\n",
      " |          the context.\n",
      " |      \n",
      " |      Yields:\n",
      " |        A context manager that specifies the default device to use for newly\n",
      " |        created ops.\n",
      " |  \n",
      " |  finalize(self)\n",
      " |      Finalizes this graph, making it read-only.\n",
      " |      \n",
      " |      After calling `g.finalize()`, no new operations can be added to\n",
      " |      `g`.  This method is used to ensure that no operations are added\n",
      " |      to a graph when it is shared between multiple threads, for example\n",
      " |      when using a `tf.train.QueueRunner`.\n",
      " |  \n",
      " |  get_all_collection_keys(self)\n",
      " |      Returns a list of collections used in this graph.\n",
      " |  \n",
      " |  get_collection(self, name, scope=None)\n",
      " |      Returns a list of values in the collection with the given `name`.\n",
      " |      \n",
      " |      This is different from `get_collection_ref()` which always returns the\n",
      " |      actual collection list if it exists in that it returns a new list each time\n",
      " |      it is called.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: The key for the collection. For example, the `GraphKeys` class\n",
      " |          contains many standard names for collections.\n",
      " |        scope: (Optional.) A string. If supplied, the resulting list is filtered\n",
      " |          to include only items whose `name` attribute matches `scope` using\n",
      " |          `re.match`. Items without a `name` attribute are never returned if a\n",
      " |          scope is supplied. The choice of `re.match` means that a `scope` without\n",
      " |          special tokens filters by prefix.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The list of values in the collection with the given `name`, or\n",
      " |        an empty list if no value has been added to that collection. The\n",
      " |        list contains the values in the order under which they were\n",
      " |        collected.\n",
      " |  \n",
      " |  get_collection_ref(self, name)\n",
      " |      Returns a list of values in the collection with the given `name`.\n",
      " |      \n",
      " |      If the collection exists, this returns the list itself, which can\n",
      " |      be modified in place to change the collection.  If the collection does\n",
      " |      not exist, it is created as an empty list and the list is returned.\n",
      " |      \n",
      " |      This is different from `get_collection()` which always returns a copy of\n",
      " |      the collection list if it exists and never creates an empty collection.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: The key for the collection. For example, the `GraphKeys` class\n",
      " |          contains many standard names for collections.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The list of values in the collection with the given `name`, or an empty\n",
      " |        list if no value has been added to that collection.\n",
      " |  \n",
      " |  get_name_scope(self)\n",
      " |      Returns the current name scope.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      with tf.name_scope('scope1'):\n",
      " |        with tf.name_scope('scope2'):\n",
      " |          print(tf.get_default_graph().get_name_scope())\n",
      " |      ```\n",
      " |      would print the string `scope1/scope2`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A string representing the current name scope.\n",
      " |  \n",
      " |  get_operation_by_name(self, name)\n",
      " |      Returns the `Operation` with the given `name`.\n",
      " |      \n",
      " |      This method may be called concurrently from multiple threads.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: The name of the `Operation` to return.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The `Operation` with the given `name`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `name` is not a string.\n",
      " |        KeyError: If `name` does not correspond to an operation in this graph.\n",
      " |  \n",
      " |  get_operations(self)\n",
      " |      Return the list of operations in the graph.\n",
      " |      \n",
      " |      You can modify the operations in place, but modifications\n",
      " |      to the list such as inserts/delete have no effect on the\n",
      " |      list of operations known to the graph.\n",
      " |      \n",
      " |      This method may be called concurrently from multiple threads.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of Operations.\n",
      " |  \n",
      " |  get_tensor_by_name(self, name)\n",
      " |      Returns the `Tensor` with the given `name`.\n",
      " |      \n",
      " |      This method may be called concurrently from multiple threads.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: The name of the `Tensor` to return.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The `Tensor` with the given `name`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `name` is not a string.\n",
      " |        KeyError: If `name` does not correspond to a tensor in this graph.\n",
      " |  \n",
      " |  gradient_override_map(self, op_type_map)\n",
      " |      EXPERIMENTAL: A context manager for overriding gradient functions.\n",
      " |      \n",
      " |      This context manager can be used to override the gradient function\n",
      " |      that will be used for ops within the scope of the context.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      @tf.RegisterGradient(\"CustomSquare\")\n",
      " |      def _custom_square_grad(op, grad):\n",
      " |        # ...\n",
      " |      \n",
      " |      with tf.Graph().as_default() as g:\n",
      " |        c = tf.constant(5.0)\n",
      " |        s_1 = tf.square(c)  # Uses the default gradient for tf.square.\n",
      " |        with g.gradient_override_map({\"Square\": \"CustomSquare\"}):\n",
      " |          s_2 = tf.square(s_2)  # Uses _custom_square_grad to compute the\n",
      " |                                # gradient of s_2.\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        op_type_map: A dictionary mapping op type strings to alternative op\n",
      " |          type strings.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A context manager that sets the alternative op type to be used for one\n",
      " |        or more ops created in that context.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `op_type_map` is not a dictionary mapping strings to\n",
      " |          strings.\n",
      " |  \n",
      " |  is_feedable(self, tensor)\n",
      " |      Returns `True` if and only if `tensor` is feedable.\n",
      " |  \n",
      " |  is_fetchable(self, tensor_or_op)\n",
      " |      Returns `True` if and only if `tensor_or_op` is fetchable.\n",
      " |  \n",
      " |  name_scope(self, name)\n",
      " |      Returns a context manager that creates hierarchical names for operations.\n",
      " |      \n",
      " |      A graph maintains a stack of name scopes. A `with name_scope(...):`\n",
      " |      statement pushes a new name onto the stack for the lifetime of the context.\n",
      " |      \n",
      " |      The `name` argument will be interpreted as follows:\n",
      " |      \n",
      " |      * A string (not ending with '/') will create a new name scope, in which\n",
      " |        `name` is appended to the prefix of all operations created in the\n",
      " |        context. If `name` has been used before, it will be made unique by\n",
      " |        calling `self.unique_name(name)`.\n",
      " |      * A scope previously captured from a `with g.name_scope(...) as\n",
      " |        scope:` statement will be treated as an \"absolute\" name scope, which\n",
      " |        makes it possible to re-enter existing scopes.\n",
      " |      * A value of `None` or the empty string will reset the current name scope\n",
      " |        to the top-level (empty) name scope.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      with tf.Graph().as_default() as g:\n",
      " |        c = tf.constant(5.0, name=\"c\")\n",
      " |        assert c.op.name == \"c\"\n",
      " |        c_1 = tf.constant(6.0, name=\"c\")\n",
      " |        assert c_1.op.name == \"c_1\"\n",
      " |      \n",
      " |        # Creates a scope called \"nested\"\n",
      " |        with g.name_scope(\"nested\") as scope:\n",
      " |          nested_c = tf.constant(10.0, name=\"c\")\n",
      " |          assert nested_c.op.name == \"nested/c\"\n",
      " |      \n",
      " |          # Creates a nested scope called \"inner\".\n",
      " |          with g.name_scope(\"inner\"):\n",
      " |            nested_inner_c = tf.constant(20.0, name=\"c\")\n",
      " |            assert nested_inner_c.op.name == \"nested/inner/c\"\n",
      " |      \n",
      " |          # Create a nested scope called \"inner_1\".\n",
      " |          with g.name_scope(\"inner\"):\n",
      " |            nested_inner_1_c = tf.constant(30.0, name=\"c\")\n",
      " |            assert nested_inner_1_c.op.name == \"nested/inner_1/c\"\n",
      " |      \n",
      " |            # Treats `scope` as an absolute name scope, and\n",
      " |            # switches to the \"nested/\" scope.\n",
      " |            with g.name_scope(scope):\n",
      " |              nested_d = tf.constant(40.0, name=\"d\")\n",
      " |              assert nested_d.op.name == \"nested/d\"\n",
      " |      \n",
      " |              with g.name_scope(\"\"):\n",
      " |                e = tf.constant(50.0, name=\"e\")\n",
      " |                assert e.op.name == \"e\"\n",
      " |      ```\n",
      " |      \n",
      " |      The name of the scope itself can be captured by `with\n",
      " |      g.name_scope(...) as scope:`, which stores the name of the scope\n",
      " |      in the variable `scope`. This value can be used to name an\n",
      " |      operation that represents the overall result of executing the ops\n",
      " |      in a scope. For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.constant(...)\n",
      " |      with g.name_scope('my_layer') as scope:\n",
      " |        weights = tf.Variable(..., name=\"weights\")\n",
      " |        biases = tf.Variable(..., name=\"biases\")\n",
      " |        affine = tf.matmul(inputs, weights) + biases\n",
      " |        output = tf.nn.relu(affine, name=scope)\n",
      " |      ```\n",
      " |      \n",
      " |      NOTE: This constructor validates the given `name`. Valid scope\n",
      " |      names match one of the following regular expressions:\n",
      " |      \n",
      " |          [A-Za-z0-9.][A-Za-z0-9_.\\\\-/]* (for scopes at the root)\n",
      " |          [A-Za-z0-9_.\\\\-/]* (for other scopes)\n",
      " |      \n",
      " |      Args:\n",
      " |        name: A name for the scope.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A context manager that installs `name` as a new name scope.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If `name` is not a valid scope name, according to the rules\n",
      " |          above.\n",
      " |  \n",
      " |  prevent_feeding(self, tensor)\n",
      " |      Marks the given `tensor` as unfeedable in this graph.\n",
      " |  \n",
      " |  prevent_fetching(self, op)\n",
      " |      Marks the given `op` as unfetchable in this graph.\n",
      " |  \n",
      " |  switch_to_thread_local(self)\n",
      " |      Make device, colocation and dependencies stacks thread-local.\n",
      " |      \n",
      " |      Device, colocation and dependencies stacks are not thread-local be default.\n",
      " |      If multiple threads access them, then the state is shared.  This means that\n",
      " |      one thread may affect the behavior of another thread.\n",
      " |      \n",
      " |      After this method is called, the stacks become thread-local.  If multiple\n",
      " |      threads access them, then the state is not shared.  Each thread uses its own\n",
      " |      value; a thread doesn't affect other threads by mutating such a stack.\n",
      " |      \n",
      " |      The initial value for every thread's stack is set to the current value\n",
      " |      of the stack when `switch_to_thread_local()` was first called.\n",
      " |  \n",
      " |  unique_name(self, name, mark_as_used=True)\n",
      " |      Return a unique operation name for `name`.\n",
      " |      \n",
      " |      Note: You rarely need to call `unique_name()` directly.  Most of\n",
      " |      the time you just need to create `with g.name_scope()` blocks to\n",
      " |      generate structured names.\n",
      " |      \n",
      " |      `unique_name` is used to generate structured names, separated by\n",
      " |      `\"/\"`, to help identify operations when debugging a graph.\n",
      " |      Operation names are displayed in error messages reported by the\n",
      " |      TensorFlow runtime, and in various visualization tools such as\n",
      " |      TensorBoard.\n",
      " |      \n",
      " |      If `mark_as_used` is set to `True`, which is the default, a new\n",
      " |      unique name is created and marked as in use. If it's set to `False`,\n",
      " |      the unique name is returned without actually being marked as used.\n",
      " |      This is useful when the caller simply wants to know what the name\n",
      " |      to be created will be.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: The name for an operation.\n",
      " |        mark_as_used: Whether to mark this name as being used.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A string to be passed to `create_op()` that will be used\n",
      " |        to name the operation being created.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  building_function\n",
      " |      Returns True iff this graph represents a function.\n",
      " |  \n",
      " |  collections\n",
      " |      Returns the names of the collections known to this graph.\n",
      " |  \n",
      " |  finalized\n",
      " |      True if this graph has been finalized.\n",
      " |  \n",
      " |  graph_def_versions\n",
      " |      The GraphDef version information of this graph.\n",
      " |      \n",
      " |      For details on the meaning of each version, see\n",
      " |      [`GraphDef`](https://www.tensorflow.org/code/tensorflow/core/framework/graph.proto).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `VersionDef`.\n",
      " |  \n",
      " |  seed\n",
      " |      The graph-level random seed of this graph.\n",
      " |  \n",
      " |  version\n",
      " |      Returns a version number that increases as ops are added to the graph.\n",
      " |      \n",
      " |      Note that this is unrelated to the\n",
      " |      `tf.Graph.graph_def_versions`.\n",
      " |      \n",
      " |      Returns:\n",
      " |         An integer version that increases as ops are added to the graph.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
